{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " # this works\n",
        "\"\"\"\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "src_text = [\n",
        "    \">>fra<< this is a sentence in english that we want to translate to french\",\n",
        "    \">>por<< This should go to portuguese\",\n",
        "    \">>ind<< And this to Spanish\",\n",
        "]\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-roa\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "print(tokenizer.supported_language_codes)\n",
        "\n",
        "\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
        "\n",
        "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nuem3BCqU27j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doesnt work but is very interesting\n",
        "\n",
        "# https://www.datacamp.com/blog/exploring-bloom-guide-to-multilingual-llm\n",
        "\n",
        "\"\"\"\n",
        "!pip install transformers -q\n",
        "\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "import torch\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "\n",
        "model_ID = \"bigscience/bloom-1b7\"\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_ID, use_cache=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ID)\n",
        "set_seed(2024)\n",
        "\n",
        "\n",
        "sentence = 'How are you?'\n",
        "language = 'French'\n",
        "prompt = f'Translate the line \"{sentence}\" to {language}.\\n'\n",
        "\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
        "\n",
        "sample = model.generate(**input_ids,\n",
        "                        max_length=200, top_k=1,\n",
        "                        temperature=0, repetition_penalty=2.0)\n",
        "\n",
        "generated_story = tokenizer.decode(sample[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=80)\n",
        "\n",
        "\n",
        "formated_story = wrapper.fill(text=generated_story)\n",
        "print(formated_story)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "6lFSEjd2h99y",
        "outputId": "a7d10051-e385-4939-b05d-4003888f31c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install transformers -q\\n\\n\\n!nvidia-smi\\n\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\nimport torch\\ntorch.set_default_tensor_type(torch.cuda.FloatTensor)\\n\\n\\nmodel_ID = \"bigscience/bloom-1b7\"\\n\\n\\nmodel = AutoModelForCausalLM.from_pretrained(model_ID, use_cache=True)\\ntokenizer = AutoTokenizer.from_pretrained(model_ID)\\nset_seed(2024)\\n\\n\\nsentence = \\'How are you?\\'\\nlanguage = \\'French\\'\\nprompt = f\\'Translate the line \"{sentence}\" to {language}.\\n\\'\\n\\n\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(0)\\n\\nsample = model.generate(**input_ids,\\n                        max_length=200, top_k=1,\\n                        temperature=0, repetition_penalty=2.0)\\n\\ngenerated_story = tokenizer.decode(sample[0], skip_special_tokens=True)\\n\\n\\nimport textwrap\\nwrapper = textwrap.TextWrapper(width=80)\\n\\n\\nformated_story = wrapper.fill(text=generated_story)\\nprint(formated_story)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Facing authentication errors\n",
        "\n",
        "# https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document\n",
        "\n",
        "'''\n",
        "! pip install --upgrade --quiet  doctran\n",
        "! pip install langchain-community\n",
        "! pip install langchain-core\n",
        "! pip install python-dotenv\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'dummy_key'\n",
        "\n",
        "\n",
        "from langchain_community.document_transformers import DoctranTextTranslator\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "sample_text = \"\"\"[Generated with ChatGPT]\n",
        "\n",
        "Confidential Document - For Internal Use Only\n",
        "\n",
        "Date: July 1, 2023\n",
        "\n",
        "Subject: Updates and Discussions on Various Topics\n",
        "\n",
        "Dear Team,\n",
        "\n",
        "I hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\n",
        "\n",
        "Security and Privacy Measures\n",
        "As part of our ongoing commitment to ensure the security and privacy of our customers' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\n",
        "\n",
        "HR Updates and Employee Benefits\n",
        "Recently, we welcomed several new team members who have made significant contributions to their respective departments. I would like to recognize Jane Smith (SSN: 049-45-5928) for her outstanding performance in customer service. Jane has consistently received positive feedback from our clients. Furthermore, please remember that the open enrollment period for our employee benefits program is fast approaching. Should you have any questions or require assistance, please contact our HR representative, Michael Johnson (phone: 418-492-3850, email: michael.johnson@example.com).\n",
        "\n",
        "Marketing Initiatives and Campaigns\n",
        "Our marketing team has been actively working on developing new strategies to increase brand awareness and drive customer engagement. We would like to thank Sarah Thompson (phone: 415-555-1234) for her exceptional efforts in managing our social media platforms. Sarah has successfully increased our follower base by 20% in the past month alone. Moreover, please mark your calendars for the upcoming product launch event on July 15th. We encourage all team members to attend and support this exciting milestone for our company.\n",
        "\n",
        "Research and Development Projects\n",
        "In our pursuit of innovation, our research and development department has been working tirelessly on various projects. I would like to acknowledge the exceptional work of David Rodriguez (email: david.rodriguez@example.com) in his role as project lead. David's contributions to the development of our cutting-edge technology have been instrumental. Furthermore, we would like to remind everyone to share their ideas and suggestions for potential new projects during our monthly R&D brainstorming session, scheduled for July 10th.\n",
        "\n",
        "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n",
        "\n",
        "Thank you for your attention, and let's continue to work together to achieve our goals.\n",
        "\n",
        "Best regards,\n",
        "\n",
        "Jason Fan\n",
        "Cofounder & CEO\n",
        "Psychic\n",
        "jason@psychic.dev\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "documents = [Document(page_content=sample_text)]\n",
        "qa_translator = DoctranTextTranslator(language=\"spanish\")\n",
        "\n",
        "\n",
        "translated_document = qa_translator.transform_documents(documents)\n",
        "\n",
        "print(translated_document[0].page_content)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "dIu-yKjNohq6",
        "outputId": "975d6f1d-7750-4d26-fa81-f9609fa7669a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n! pip install --upgrade --quiet  doctran\\n! pip install langchain-community\\n! pip install langchain-core\\n! pip install python-dotenv\\n\\nimport os\\n\\nos.environ[\\'OPENAI_API_KEY\\'] = \\'dummy_key\\'\\n\\n\\nfrom langchain_community.document_transformers import DoctranTextTranslator\\nfrom langchain_core.documents import Document\\n\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\n\\nsample_text = \"\"\"[Generated with ChatGPT]\\n\\nConfidential Document - For Internal Use Only\\n\\nDate: July 1, 2023\\n\\nSubject: Updates and Discussions on Various Topics\\n\\nDear Team,\\n\\nI hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\\n\\nSecurity and Privacy Measures\\nAs part of our ongoing commitment to ensure the security and privacy of our customers\\' data, we have implemented robust measures across all our systems. We would like to commend John Doe (email: john.doe@example.com) from the IT department for his diligent work in enhancing our network security. Moving forward, we kindly remind everyone to strictly adhere to our data protection policies and guidelines. Additionally, if you come across any potential security risks or incidents, please report them immediately to our dedicated team at security@example.com.\\n\\nHR Updates and Employee Benefits\\nRecently, we welcomed several new team members who have made significant contributions to their respective departments. I would like to recognize Jane Smith (SSN: 049-45-5928) for her outstanding performance in customer service. Jane has consistently received positive feedback from our clients. Furthermore, please remember that the open enrollment period for our employee benefits program is fast approaching. Should you have any questions or require assistance, please contact our HR representative, Michael Johnson (phone: 418-492-3850, email: michael.johnson@example.com).\\n\\nMarketing Initiatives and Campaigns\\nOur marketing team has been actively working on developing new strategies to increase brand awareness and drive customer engagement. We would like to thank Sarah Thompson (phone: 415-555-1234) for her exceptional efforts in managing our social media platforms. Sarah has successfully increased our follower base by 20% in the past month alone. Moreover, please mark your calendars for the upcoming product launch event on July 15th. We encourage all team members to attend and support this exciting milestone for our company.\\n\\nResearch and Development Projects\\nIn our pursuit of innovation, our research and development department has been working tirelessly on various projects. I would like to acknowledge the exceptional work of David Rodriguez (email: david.rodriguez@example.com) in his role as project lead. David\\'s contributions to the development of our cutting-edge technology have been instrumental. Furthermore, we would like to remind everyone to share their ideas and suggestions for potential new projects during our monthly R&D brainstorming session, scheduled for July 10th.\\n\\nPlease treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\\n\\nThank you for your attention, and let\\'s continue to work together to achieve our goals.\\n\\nBest regards,\\n\\nJason Fan\\nCofounder & CEO\\nPsychic\\njason@psychic.dev\\n\"\"\"\\n\\n\\ndocuments = [Document(page_content=sample_text)]\\nqa_translator = DoctranTextTranslator(language=\"spanish\")\\n\\n\\ntranslated_document = qa_translator.transform_documents(documents)\\n\\nprint(translated_document[0].page_content)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@theprogramminggeek/harnessing-ai-for-seamless-language-translation-in-python-0d382beae579\n",
        "\"\"\"\n",
        "\n",
        "! pip install --upgrade google-cloud-translate\n",
        "\n",
        "from google.cloud import translate_v2 as translate\n",
        "import os\n",
        "\n",
        "# Set up authentication:\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path-to-your-service-account-file.json'\n",
        "\n",
        "client = translate.Client()\n",
        "\n",
        "def translate_text(text, target='en'):\n",
        "    result = client.translate(\n",
        "        text,\n",
        "        target_language=target\n",
        "    )\n",
        "    print(\"Translation: \" + result['translatedText'])\n",
        "\n",
        "translate_text('Hola mundo', 'en') # Example usage\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "C9JDCu8UtjZ2",
        "outputId": "e1b8d4bf-1fc7-44fc-b107-e01a3ef21f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n! pip install --upgrade google-cloud-translate\\n\\nfrom google.cloud import translate_v2 as translate\\nimport os\\n\\n# Set up authentication:\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'path-to-your-service-account-file.json\\'\\n\\nclient = translate.Client()\\n\\ndef translate_text(text, target=\\'en\\'):\\n    result = client.translate(\\n        text,\\n        target_language=target\\n    )\\n    print(\"Translation: \" + result[\\'translatedText\\'])\\n\\ntranslate_text(\\'Hola mundo\\', \\'en\\') # Example usage\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# works quite well\n",
        "\n",
        "# https://huggingface.co/docs/transformers/en/multilingual\n",
        "\n",
        "\"\"\"\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "article_hi = \"संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है\"\n",
        "article_ar = \"الأمين العام للأمم المتحدة يقول إنه لا يوجد حل عسكري في سوريا.\"\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "# translate Hindi to French\n",
        "tokenizer.src_lang = \"hi_IN\"\n",
        "encoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\n",
        "generated_tokens = model.generate(\n",
        "    **encoded_hi,\n",
        "    forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"]\n",
        ")\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
        "# => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire dans la Syrie.\"\n",
        "\n",
        "# translate Arabic to English\n",
        "tokenizer.src_lang = \"ar_AR\"\n",
        "encoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\n",
        "generated_tokens = model.generate(\n",
        "    **encoded_ar,\n",
        "    forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
        ")\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vrEnZacGwYjj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "a59ea914-16f8-4f12-dce5-f0db9f099ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\\n\\narticle_hi = \"संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है\"\\narticle_ar = \"الأمين العام للأمم المتحدة يقول إنه لا يوجد حل عسكري في سوريا.\"\\n\\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\\n\\n# translate Hindi to French\\ntokenizer.src_lang = \"hi_IN\"\\nencoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\\ngenerated_tokens = model.generate(\\n    **encoded_hi,\\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"]\\n)\\nprint(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\\n# => \"Le chef de l \\'ONU affirme qu \\'il n \\'y a pas de solution militaire dans la Syrie.\"\\n\\n# translate Arabic to English\\ntokenizer.src_lang = \"ar_AR\"\\nencoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\\ngenerated_tokens = model.generate(\\n    **encoded_ar,\\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\\n)\\nprint(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/facebook/nllb-moe-54b\n",
        "\"\"\"\n",
        "! pip install transformers -q\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n",
        "\n",
        "batched_input = [\n",
        "'We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.',\n",
        "\"Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days.\"\n",
        "\"Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes.\"\n",
        "\"On Monday, Sara Danius, permanent secretary of the Nobel Committee for Literature at the Swedish Academy, publicly announced during a radio program on Sveriges Radio in Sweden the committee, unable to reach Bob Dylan directly about winning the 2016 Nobel Prize in Literature, had abandoned its efforts to reach him.\",\n",
        "'Danius said, \"Right now we are doing nothing. I have called and sent emails to his closest collaborator and received very friendly replies. For now, that is certainly enough.\"',\n",
        "\"Previously, Ring's CEO, Jamie Siminoff, remarked the company started when his doorbell wasn't audible from his shop in his garage.\",\n",
        "]\n",
        "\n",
        "inputs = tokenizer(article, return_tensors=\"pt\", padding = True)\n",
        "\n",
        "translated_tokens = model.generate(\n",
        "\n",
        "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"hin_Deva\"]\n",
        "\n",
        ")\n",
        "\n",
        "tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "H4DeZOcq6pGr",
        "outputId": "fc48a935-1480-4cb6-e9b5-00ef328e7e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n! pip install transformers -q\\n\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\\n\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\\n\\nbatched_input = [\\n\\'We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\\',\\n\"Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days.\"\\n\"Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes.\"\\n\"On Monday, Sara Danius, permanent secretary of the Nobel Committee for Literature at the Swedish Academy, publicly announced during a radio program on Sveriges Radio in Sweden the committee, unable to reach Bob Dylan directly about winning the 2016 Nobel Prize in Literature, had abandoned its efforts to reach him.\",\\n\\'Danius said, \"Right now we are doing nothing. I have called and sent emails to his closest collaborator and received very friendly replies. For now, that is certainly enough.\"\\',\\n\"Previously, Ring\\'s CEO, Jamie Siminoff, remarked the company started when his doorbell wasn\\'t audible from his shop in his garage.\",\\n]\\n\\ninputs = tokenizer(article, return_tensors=\"pt\", padding = True)\\n\\ntranslated_tokens = model.generate(\\n\\n    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"hin_Deva\"]\\n\\n)\\n\\ntokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/facebookresearch/fairseq/issues/4547\n",
        "\n",
        "'''\n",
        "Language\tFLORES-200 code\n",
        "Acehnese (Arabic script)\tace_Arab\n",
        "Acehnese (Latin script)\tace_Latn\n",
        "Mesopotamian Arabic\tacm_Arab\n",
        "Ta’izzi-Adeni Arabic\tacq_Arab\n",
        "Tunisian Arabic\taeb_Arab\n",
        "Afrikaans\tafr_Latn\n",
        "South Levantine Arabic\tajp_Arab\n",
        "Akan\taka_Latn\n",
        "Amharic\tamh_Ethi\n",
        "North Levantine Arabic\tapc_Arab\n",
        "Modern Standard Arabic\tarb_Arab\n",
        "Modern Standard Arabic (Romanized)\tarb_Latn\n",
        "Najdi Arabic\tars_Arab\n",
        "Moroccan Arabic\tary_Arab\n",
        "Egyptian Arabic\tarz_Arab\n",
        "Assamese\tasm_Beng\n",
        "Asturian\tast_Latn\n",
        "Awadhi\tawa_Deva\n",
        "Central Aymara\tayr_Latn\n",
        "South Azerbaijani\tazb_Arab\n",
        "North Azerbaijani\tazj_Latn\n",
        "Bashkir\tbak_Cyrl\n",
        "Bambara\tbam_Latn\n",
        "Balinese\tban_Latn\n",
        "Belarusian\tbel_Cyrl\n",
        "Bemba\tbem_Latn\n",
        "Bengali\tben_Beng\n",
        "Bhojpuri\tbho_Deva\n",
        "Banjar (Arabic script)\tbjn_Arab\n",
        "Banjar (Latin script)\tbjn_Latn\n",
        "Standard Tibetan\tbod_Tibt\n",
        "Bosnian\tbos_Latn\n",
        "Buginese\tbug_Latn\n",
        "Bulgarian\tbul_Cyrl\n",
        "Catalan\tcat_Latn\n",
        "Cebuano\tceb_Latn\n",
        "Czech\tces_Latn\n",
        "Chokwe\tcjk_Latn\n",
        "Central Kurdish\tckb_Arab\n",
        "Crimean Tatar\tcrh_Latn\n",
        "Welsh\tcym_Latn\n",
        "Danish\tdan_Latn\n",
        "German\tdeu_Latn\n",
        "Southwestern Dinka\tdik_Latn\n",
        "Dyula\tdyu_Latn\n",
        "Dzongkha\tdzo_Tibt\n",
        "Greek\tell_Grek\n",
        "English\teng_Latn\n",
        "Esperanto\tepo_Latn\n",
        "Estonian\test_Latn\n",
        "Basque\teus_Latn\n",
        "Ewe\tewe_Latn\n",
        "Faroese\tfao_Latn\n",
        "Fijian\tfij_Latn\n",
        "Finnish\tfin_Latn\n",
        "Fon\tfon_Latn\n",
        "French\tfra_Latn\n",
        "Friulian\tfur_Latn\n",
        "Nigerian Fulfulde\tfuv_Latn\n",
        "Scottish Gaelic\tgla_Latn\n",
        "Irish\tgle_Latn\n",
        "Galician\tglg_Latn\n",
        "Guarani\tgrn_Latn\n",
        "Gujarati\tguj_Gujr\n",
        "Haitian Creole\that_Latn\n",
        "Hausa\thau_Latn\n",
        "Hebrew\theb_Hebr\n",
        "Hindi\thin_Deva\n",
        "Chhattisgarhi\thne_Deva\n",
        "Croatian\thrv_Latn\n",
        "Hungarian\thun_Latn\n",
        "Armenian\thye_Armn\n",
        "Igbo\tibo_Latn\n",
        "Ilocano\tilo_Latn\n",
        "Indonesian\tind_Latn\n",
        "Icelandic\tisl_Latn\n",
        "Italian\tita_Latn\n",
        "Javanese\tjav_Latn\n",
        "Japanese\tjpn_Jpan\n",
        "Kabyle\tkab_Latn\n",
        "Jingpho\tkac_Latn\n",
        "Kamba\tkam_Latn\n",
        "Kannada\tkan_Knda\n",
        "Kashmiri (Arabic script)\tkas_Arab\n",
        "Kashmiri (Devanagari script)\tkas_Deva\n",
        "Georgian\tkat_Geor\n",
        "Central Kanuri (Arabic script)\tknc_Arab\n",
        "Central Kanuri (Latin script)\tknc_Latn\n",
        "Kazakh\tkaz_Cyrl\n",
        "Kabiyè\tkbp_Latn\n",
        "Kabuverdianu\tkea_Latn\n",
        "Khmer\tkhm_Khmr\n",
        "Kikuyu\tkik_Latn\n",
        "Kinyarwanda\tkin_Latn\n",
        "Kyrgyz\tkir_Cyrl\n",
        "Kimbundu\tkmb_Latn\n",
        "Northern Kurdish\tkmr_Latn\n",
        "Kikongo\tkon_Latn\n",
        "Korean\tkor_Hang\n",
        "Lao\tlao_Laoo\n",
        "Ligurian\tlij_Latn\n",
        "Limburgish\tlim_Latn\n",
        "Lingala\tlin_Latn\n",
        "Lithuanian\tlit_Latn\n",
        "Lombard\tlmo_Latn\n",
        "Latgalian\tltg_Latn\n",
        "Luxembourgish\tltz_Latn\n",
        "Luba-Kasai\tlua_Latn\n",
        "Ganda\tlug_Latn\n",
        "Luo\tluo_Latn\n",
        "Mizo\tlus_Latn\n",
        "Standard Latvian\tlvs_Latn\n",
        "Magahi\tmag_Deva\n",
        "Maithili\tmai_Deva\n",
        "Malayalam\tmal_Mlym\n",
        "Marathi\tmar_Deva\n",
        "Minangkabau (Arabic script)\tmin_Arab\n",
        "Minangkabau (Latin script)\tmin_Latn\n",
        "Macedonian\tmkd_Cyrl\n",
        "Plateau Malagasy\tplt_Latn\n",
        "Maltese\tmlt_Latn\n",
        "Meitei (Bengali script)\tmni_Beng\n",
        "Halh Mongolian\tkhk_Cyrl\n",
        "Mossi\tmos_Latn\n",
        "Maori\tmri_Latn\n",
        "Burmese\tmya_Mymr\n",
        "Dutch\tnld_Latn\n",
        "Norwegian Nynorsk\tnno_Latn\n",
        "Norwegian Bokmål\tnob_Latn\n",
        "Nepali\tnpi_Deva\n",
        "Northern Sotho\tnso_Latn\n",
        "Nuer\tnus_Latn\n",
        "Nyanja\tnya_Latn\n",
        "Occitan\toci_Latn\n",
        "West Central Oromo\tgaz_Latn\n",
        "Odia\tory_Orya\n",
        "Pangasinan\tpag_Latn\n",
        "Eastern Panjabi\tpan_Guru\n",
        "Papiamento\tpap_Latn\n",
        "Western Persian\tpes_Arab\n",
        "Polish\tpol_Latn\n",
        "Portuguese\tpor_Latn\n",
        "Dari\tprs_Arab\n",
        "Southern Pashto\tpbt_Arab\n",
        "Ayacucho Quechua\tquy_Latn\n",
        "Romanian\tron_Latn\n",
        "Rundi\trun_Latn\n",
        "Russian\trus_Cyrl\n",
        "Sango\tsag_Latn\n",
        "Sanskrit\tsan_Deva\n",
        "Santali\tsat_Olck\n",
        "Sicilian\tscn_Latn\n",
        "Shan\tshn_Mymr\n",
        "Sinhala\tsin_Sinh\n",
        "Slovak\tslk_Latn\n",
        "Slovenian\tslv_Latn\n",
        "Samoan\tsmo_Latn\n",
        "Shona\tsna_Latn\n",
        "Sindhi\tsnd_Arab\n",
        "Somali\tsom_Latn\n",
        "Southern Sotho\tsot_Latn\n",
        "Spanish\tspa_Latn\n",
        "Tosk Albanian\tals_Latn\n",
        "Sardinian\tsrd_Latn\n",
        "Serbian\tsrp_Cyrl\n",
        "Swati\tssw_Latn\n",
        "Sundanese\tsun_Latn\n",
        "Swedish\tswe_Latn\n",
        "Swahili\tswh_Latn\n",
        "Silesian\tszl_Latn\n",
        "Tamil\ttam_Taml\n",
        "Tatar\ttat_Cyrl\n",
        "Telugu\ttel_Telu\n",
        "Tajik\ttgk_Cyrl\n",
        "Tagalog\ttgl_Latn\n",
        "Thai\ttha_Thai\n",
        "Tigrinya\ttir_Ethi\n",
        "Tamasheq (Latin script)\ttaq_Latn\n",
        "Tamasheq (Tifinagh script)\ttaq_Tfng\n",
        "Tok Pisin\ttpi_Latn\n",
        "Tswana\ttsn_Latn\n",
        "Tsonga\ttso_Latn\n",
        "Turkmen\ttuk_Latn\n",
        "Tumbuka\ttum_Latn\n",
        "Turkish\ttur_Latn\n",
        "Twi\ttwi_Latn\n",
        "Central Atlas Tamazight\ttzm_Tfng\n",
        "Uyghur\tuig_Arab\n",
        "Ukrainian\tukr_Cyrl\n",
        "Umbundu\tumb_Latn\n",
        "Urdu\turd_Arab\n",
        "Northern Uzbek\tuzn_Latn\n",
        "Venetian\tvec_Latn\n",
        "Vietnamese\tvie_Latn\n",
        "Waray\twar_Latn\n",
        "Wolof\twol_Latn\n",
        "Xhosa\txho_Latn\n",
        "Eastern Yiddish\tydd_Hebr\n",
        "Yoruba\tyor_Latn\n",
        "Yue Chinese\tyue_Hant\n",
        "Chinese (Simplified)\tzho_Hans\n",
        "Chinese (Traditional)\tzho_Hant\n",
        "Standard Malay\tzsm_Latn\n",
        "Zulu\tzul_Latn\n",
        "'''\n",
        "\n",
        "! pip install transformers -q\n",
        "# or do ! pip install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# available models: 'facebook/nllb-200-distilled-600M', 'facebook/nllb-200-1.3B', 'facebook/nllb-200-distilled-1.3B', 'facebook/nllb-200-3.3B'\n",
        "model_name = 'facebook/nllb-200-distilled-1.3B'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekk3TTNh0Vy-",
        "outputId": "921dd816-e39d-43cd-ebf9-4328fc5072e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def break_paragraph(paragraph, chunk_size):\n",
        "    chunks = []\n",
        "    start_index = 0\n",
        "    lists = paragraph.split(' ')\n",
        "    # print(lists)\n",
        "    csize = len(lists)//chunk_size\n",
        "    while(start_index+csize<len(lists)):\n",
        "        chunk = lists[start_index:start_index+csize]\n",
        "        chunks.append(chunk)\n",
        "        start_index += csize\n",
        "    return chunks\n",
        "\n",
        "def process_chunks(chunks):\n",
        "    phrases = []\n",
        "    for c in chunks:\n",
        "        phrases.append(\" \".join(c))\n",
        "    return phrases\n",
        "# Example usage:\n",
        "# paragraph = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
        "paragraph = 'This is a very long paragrpah, detailing every account of what may or may not happen in life. Life as we know it, is not linear but rather karmic and cyclic.'\n",
        "chunk_sizes = [5, 10, 15, 20]\n",
        "pr_par = []\n",
        "for chunk_size in chunk_sizes:\n",
        "    chunks = break_paragraph(paragraph, chunk_size)\n",
        "    # print(chunks)\n",
        "    processed_paragraph = process_chunks(chunks)\n",
        "    print(processed_paragraph)\n",
        "    pr_par.append(processed_paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2aYempyxdqc",
        "outputId": "3201c70c-b258-4450-9384-1b709aefd2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a very long paragrpah,', 'detailing every account of what may', 'or may not happen in life.', 'Life as we know it, is', 'not linear but rather karmic and']\n",
            "['This is a', 'very long paragrpah,', 'detailing every account', 'of what may', 'or may not', 'happen in life.', 'Life as we', 'know it, is', 'not linear but', 'rather karmic and']\n",
            "['This is', 'a very', 'long paragrpah,', 'detailing every', 'account of', 'what may', 'or may', 'not happen', 'in life.', 'Life as', 'we know', 'it, is', 'not linear', 'but rather', 'karmic and']\n",
            "['This', 'is', 'a', 'very', 'long', 'paragrpah,', 'detailing', 'every', 'account', 'of', 'what', 'may', 'or', 'may', 'not', 'happen', 'in', 'life.', 'Life', 'as', 'we', 'know', 'it,', 'is', 'not', 'linear', 'but', 'rather', 'karmic', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = translate(source, 'hin_Deva')"
      ],
      "metadata": {
        "id": "gQp_guGaw4ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(translate_class, pr_par):\n",
        "    infer_text = ''\n",
        "    for line in pr_par:\n",
        "        infer_text += ' '+ translate_class.infer(text=line)\n",
        "    return infer_text\n",
        "\n",
        "outputs = []\n",
        "for i,pr in enumerate(pr_par):\n",
        "    time1 = time.perf_counter()\n",
        "    outputs.append(inference(test, pr_par[i]))\n",
        "    time2 = time.perf_counter()\n",
        "    print(f'Time to process text chunked into sizes of {chunk_sizes[i]}: {time2-time1}')\n",
        "    print(f'Translation:\\n{outputs[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbZf-475x9_H",
        "outputId": "bbad550b-01ed-48b1-b947-bafff6c23bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to process text chunked into sizes of 5: 42.597767757000156\n",
            "Translation:\n",
            " यह एक बहुत ही लम्बा पैराग्राफ है, जो कुछ भी हो सकता है, उसका विवरण देते हुए या जीवन में नहीं हो सकता है। जीवन जैसा कि हम इसे जानते हैं, रैखिक नहीं बल्कि कर्मिक और\n",
            "Time to process text chunked into sizes of 10: 34.07475969899997\n",
            "Translation:\n",
            " यह एक बहुत लम्बी पैराग्राफ, हर एक ख़बर का विवरण देते हुए जो हो सकता है या नहीं भी हो सकता है जीवन में होता है। जीवन जैसा हम यह पता है, है रैखिक नहीं बल्कि बल्कि कर्मिक और\n",
            "Time to process text chunked into sizes of 15: 38.664735759999985\n",
            "Translation:\n",
            " यह है बहुत लम्बी पैराग्राफ, प्रत्येक विवरण खाता क्या हो सकता है या हो सकता है नहीं होता जीवन में। जीवन के रूप में हम जानते हैं यह, है न रैखिक बल्कि कर्मिक और\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQbhH4e9zHI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class translate():\n",
        "  def __init__(self, source, target):\n",
        "    self.source = source\n",
        "    self.target = target\n",
        "    self.pipeline = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=source, tgt_lang=target)\n",
        "\n",
        "  def infer(self, text, max_length=1000):\n",
        "    output = self.pipeline(text, max_length=max_length)\n",
        "    translated_text = output[0]['translation_text']\n",
        "    return translated_text\n",
        "\n",
        "source = 'eng_Latn'\n",
        "# target_1 = 'hin_Deva'\n",
        "# target_2 = 'urd_Arab'\n",
        "# target_3 = 'mar_Deva'\n",
        "# target_4 = 'fra_Latn'\n",
        "# target_5 = 'deu_Latn'\n",
        "\n",
        "target_languages = ['hin_Deva', 'urd_Arab', 'mar_Deva', 'fra_Latn', 'deu_Latn']\n",
        "\n",
        "text = 'This is a very long paragrpah, detailing every account of what may or may not happen in life. Life as we know it, is not linear but rather karmic and cyclic.'\n",
        "\n",
        "\n",
        "\n",
        "# obj1 = translate(source, target_1)\n",
        "# obj2 = translate(source, target_2)\n",
        "# obj3 = translate(source, target_3)\n",
        "# obj4 = translate(source, target_4)\n",
        "# obj5 = translate(source, target_5)\n",
        "\n",
        "class_obj_list = []\n",
        "for i in range(5):\n",
        "  class_obj_list.append(translate(source, target_languages[i]))\n",
        "\n",
        "def make_life_easy(class_obj):\n",
        "  time1 = time.perf_counter()\n",
        "  print(f\"Translated to '{class_obj.target}' Translation is '{class_obj.infer(text)}'\")\n",
        "  print(f\"Inference time = {time.perf_counter() - time1}\")\n",
        "  print()\n",
        "\n",
        "\n",
        "# make_life_easy(obj1)\n",
        "# make_life_easy(obj2)\n",
        "# make_life_easy(obj3)\n",
        "# make_life_easy(obj4)\n",
        "# make_life_easy(obj5)\n",
        "\n",
        "# for i in range(5):\n",
        "#   make_life_easy(class_obj_list[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "sSA6gP_p47_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}